{"cells":[{"cell_type":"markdown","metadata":{"id":"CppIQlPhhwhs"},"source":["# Generate images from text prompts with VQGAN and CLIP (z+quantize method).\n","\n","Originally made by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings). The original BigGAN+CLIP method was by https://twitter.com/advadnoun.\n"," Added some explanations and modifications by Eleiber#8347, pooling trick by Crimeacs#8222 (https://twitter.com/EarthML1) and the GUI was made with the help of Abulafia#3734.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TkUfzT60ZZ9q","outputId":"b577d2b0-0611-46b5-aab8-50140a2f3b7e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Wed Oct 16 11:34:21 2024       \r\n","+---------------------------------------------------------------------------------------+\r\n","| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\r\n","|-----------------------------------------+----------------------+----------------------+\r\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n","|                                         |                      |               MIG M. |\r\n","|=========================================+======================+======================|\r\n","|   0  NVIDIA GeForce RTX 3090        Off | 00000000:01:00.0  On |                  N/A |\r\n","| 30%   48C    P5              80W / 370W |    578MiB / 24576MiB |     22%      Default |\r\n","|                                         |                      |                  N/A |\r\n","+-----------------------------------------+----------------------+----------------------+\r\n","                                                                                         \r\n","+---------------------------------------------------------------------------------------+\r\n","| Processes:                                                                            |\r\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n","|        ID   ID                                                             Usage      |\r\n","|=======================================================================================|\r\n","|    0   N/A  N/A      1960      G   /usr/lib/xorg/Xorg                          104MiB |\r\n","|    0   N/A  N/A      2130    C+G   ...libexec/gnome-remote-desktop-daemon      258MiB |\r\n","|    0   N/A  N/A      2183      G   /usr/bin/gnome-shell                         90MiB |\r\n","|    0   N/A  N/A      4301      G   ...seed-version=20241015-180122.530000      107MiB |\r\n","+---------------------------------------------------------------------------------------+\r\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VA1PHoJrRiK9","outputId":"572a08a7-87f8-45ba-cea3-952df17e934c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: ftfy in ./miniconda3/lib/python3.12/site-packages (6.3.0)\r\n","Requirement already satisfied: regex in ./miniconda3/lib/python3.12/site-packages (2024.9.11)\r\n","Requirement already satisfied: tqdm in ./miniconda3/lib/python3.12/site-packages (4.66.5)\r\n","Requirement already satisfied: omegaconf in ./miniconda3/lib/python3.12/site-packages (2.3.0)\r\n","Requirement already satisfied: pytorch-lightning in ./miniconda3/lib/python3.12/site-packages (2.4.0)\r\n","Requirement already satisfied: wcwidth in ./miniconda3/lib/python3.12/site-packages (from ftfy) (0.2.13)\r\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in ./miniconda3/lib/python3.12/site-packages (from omegaconf) (4.9.3)\n","Requirement already satisfied: PyYAML>=5.1.0 in ./miniconda3/lib/python3.12/site-packages (from omegaconf) (6.0.2)\n","Requirement already satisfied: torch>=2.1.0 in ./miniconda3/lib/python3.12/site-packages (from pytorch-lightning) (2.4.1)\n","Requirement already satisfied: fsspec>=2022.5.0 in ./miniconda3/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.1)\n","Requirement already satisfied: torchmetrics>=0.7.0 in ./miniconda3/lib/python3.12/site-packages (from pytorch-lightning) (1.4.3)\n","Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.12/site-packages (from pytorch-lightning) (23.2)\n","Requirement already satisfied: typing-extensions>=4.4.0 in ./miniconda3/lib/python3.12/site-packages (from pytorch-lightning) (4.12.2)\n","Requirement already satisfied: lightning-utilities>=0.10.0 in ./miniconda3/lib/python3.12/site-packages (from pytorch-lightning) (0.11.8)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./miniconda3/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.8)\n","Requirement already satisfied: setuptools in ./miniconda3/lib/python3.12/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (69.5.1)\n","Requirement already satisfied: filelock in ./miniconda3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning) (3.13.1)\n","Requirement already satisfied: sympy in ./miniconda3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning) (1.12)\n","Requirement already satisfied: networkx in ./miniconda3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning) (3.2.1)\n","Requirement already satisfied: jinja2 in ./miniconda3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning) (3.1.3)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./miniconda3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./miniconda3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./miniconda3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./miniconda3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./miniconda3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./miniconda3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./miniconda3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./miniconda3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./miniconda3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./miniconda3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./miniconda3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning) (12.1.105)\n","Requirement already satisfied: triton==3.0.0 in ./miniconda3/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning) (3.0.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in ./miniconda3/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.0->pytorch-lightning) (12.3.101)\n","Requirement already satisfied: numpy>1.20.0 in ./miniconda3/lib/python3.12/site-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.3)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in ./miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in ./miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.13.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in ./miniconda3/lib/python3.12/site-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in ./miniconda3/lib/python3.12/site-packages (from sympy->torch>=2.1.0->pytorch-lightning) (1.3.0)\n","Requirement already satisfied: idna>=2.0 in ./miniconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.7)\n","Requirement already satisfied: kornia in ./miniconda3/lib/python3.12/site-packages (0.7.3)\n","Requirement already satisfied: kornia-rs>=0.1.0 in ./miniconda3/lib/python3.12/site-packages (from kornia) (0.1.5)\n","Requirement already satisfied: packaging in ./miniconda3/lib/python3.12/site-packages (from kornia) (23.2)\n","Requirement already satisfied: torch>=1.9.1 in ./miniconda3/lib/python3.12/site-packages (from kornia) (2.4.1)\n","Requirement already satisfied: filelock in ./miniconda3/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (4.12.2)\n","Requirement already satisfied: sympy in ./miniconda3/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (1.12)\n","Requirement already satisfied: networkx in ./miniconda3/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (3.2.1)\n","Requirement already satisfied: jinja2 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (3.1.3)\n","Requirement already satisfied: fsspec in ./miniconda3/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (2024.6.1)\n","Requirement already satisfied: setuptools in ./miniconda3/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (69.5.1)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (12.1.105)\n","Requirement already satisfied: triton==3.0.0 in ./miniconda3/lib/python3.12/site-packages (from torch>=1.9.1->kornia) (3.0.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in ./miniconda3/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9.1->kornia) (12.3.101)\n","Requirement already satisfied: MarkupSafe>=2.0 in ./miniconda3/lib/python3.12/site-packages (from jinja2->torch>=1.9.1->kornia) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in ./miniconda3/lib/python3.12/site-packages (from sympy->torch>=1.9.1->kornia) (1.3.0)\n","Requirement already satisfied: imageio-ffmpeg in ./miniconda3/lib/python3.12/site-packages (0.5.1)\n","Requirement already satisfied: setuptools in ./miniconda3/lib/python3.12/site-packages (from imageio-ffmpeg) (69.5.1)\n","Requirement already satisfied: einops in ./miniconda3/lib/python3.12/site-packages (0.8.0)\n"]}],"source":["\n","!pip install ftfy regex tqdm omegaconf pytorch-lightning\n","!pip install kornia\n","!pip install imageio-ffmpeg\n","!pip install einops\n"]},{"cell_type":"markdown","metadata":{"id":"nTg77tNuF7Og"},"source":["By default, the notebook downloads the 1024 and 16384 models from ImageNet. There are others like COCO-Stuff, WikiArt or S-FLCKR, which are heavy, and if you are not going to use them it would be useless to download them, so if you want to use them, simply remove the numerals at the beginning of the lines depending on the model you want (the model name is at the end of the lines)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DheKdcAKQxZv","outputId":"0450921d-9d5b-404b-c73e-1a3c1d5eb66e"},"outputs":[{"data":{"text/plain":["'/home/avidmech'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_IbjCf5M-0K9","outputId":"1c293fdb-4cfc-4052-b050-9dec78041378"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/avidmech/taming-transformers\n"]}],"source":["cd taming-transformers/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NK7Q3HrW_JSy","outputId":"cbca89be-2bf3-4318-dcf4-d7b8fb41215a"},"outputs":[{"name":"stdout","output_type":"stream","text":[" \u001b[0m\u001b[01;34massets\u001b[0m/                           README.md\r\n"," \u001b[01;34mCLIP\u001b[0m/                             \u001b[01;34mscripts\u001b[0m/\r\n"," \u001b[01;34mconfigs\u001b[0m/                          setup.py\r\n"," \u001b[01;34mdata\u001b[0m/                             \u001b[01;34msteps\u001b[0m/\r\n"," environment.yaml                  \u001b[01;34mtaming\u001b[0m/\r\n"," License.txt                       \u001b[01;34mtaming_transformers.egg-info\u001b[0m/\r\n"," main.py                           vqgan_imagenet_f16_16384.ckpt\r\n","\u001b[01;35m'photo_1403-07-24 12.08.28.jpeg'\u001b[0m   vqgan_imagenet_f16_16384.yaml\r\n"," \u001b[01;35mprogress.png\u001b[0m                      vqgan_openimages_f16_8192.ckpt\r\n"," \u001b[01;34m__pycache__\u001b[0m/                      vqgan_openimages_f16_8192.yaml\r\n"]}],"source":["%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O-lp7T669urh","outputId":"c76157da-296d-46d5-df5d-dc313768128a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\r\n","Requirement already satisfied: torch in /home/avidmech/miniconda3/lib/python3.12/site-packages (2.4.1)\n","Requirement already satisfied: torchvision in /home/avidmech/miniconda3/lib/python3.12/site-packages (0.19.1)\n","Requirement already satisfied: torchaudio in /home/avidmech/miniconda3/lib/python3.12/site-packages (2.4.1)\n","Requirement already satisfied: filelock in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n","Requirement already satisfied: setuptools in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torch) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==3.0.0 in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torch) (3.0.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/avidmech/miniconda3/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n","Requirement already satisfied: numpy in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torchvision) (1.26.3)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/avidmech/miniconda3/lib/python3.12/site-packages (from torchvision) (11.0.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /home/avidmech/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /home/avidmech/miniconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: ftfy in /home/avidmech/miniconda3/lib/python3.12/site-packages (6.3.0)\n","Requirement already satisfied: regex in /home/avidmech/miniconda3/lib/python3.12/site-packages (2024.9.11)\n","Requirement already satisfied: tqdm in /home/avidmech/miniconda3/lib/python3.12/site-packages (4.66.5)\n","Requirement already satisfied: omegaconf in /home/avidmech/miniconda3/lib/python3.12/site-packages (2.3.0)\n","Requirement already satisfied: pyyaml in /home/avidmech/miniconda3/lib/python3.12/site-packages (6.0.2)\n","Requirement already satisfied: munch in /home/avidmech/miniconda3/lib/python3.12/site-packages (4.0.0)\n","Requirement already satisfied: wcwidth in /home/avidmech/miniconda3/lib/python3.12/site-packages (from ftfy) (0.2.13)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/avidmech/miniconda3/lib/python3.12/site-packages (from omegaconf) (4.9.3)\n","Requirement already satisfied: pillow in /home/avidmech/miniconda3/lib/python3.12/site-packages (11.0.0)\n","Requirement already satisfied: einops in /home/avidmech/miniconda3/lib/python3.12/site-packages (0.8.0)\n"]}],"source":["!pip install --upgrade torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n","!pip install ftfy regex tqdm omegaconf pyyaml munch\n","!pip install --upgrade pillow\n","!pip install einops\n","# Apply a fix for the \"_six\" import issue by changing the import statement\n","# in file \"taming-transformers/taming/data/utils.py\"\n","!sed -i 's/from torch._six import string_classes/from collections.abc import Iterable/g' taming/data/utils.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99XaAGUoAB9t","outputId":"38953786-b7c2-4b17-a442-c0577f920fbb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: imageio in /home/avidmech/miniconda3/lib/python3.12/site-packages (2.34.1)\n","Requirement already satisfied: numpy in /home/avidmech/miniconda3/lib/python3.12/site-packages (from imageio) (1.26.3)\n","Requirement already satisfied: pillow>=8.3.2 in /home/avidmech/miniconda3/lib/python3.12/site-packages (from imageio) (11.0.0)\n"]}],"source":["!pip install imageio"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EXMSuW2EQWsd","outputId":"04d9ce6e-f1c4-4b3b-b16d-b07a039553f7"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/avidmech/miniconda3/lib/python3.12/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n","  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n","2024-10-16 11:35:14.248541: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-10-16 11:35:14.276087: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-10-16 11:35:14.851670: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["# @title Load libraries and variables\n","\n","import argparse\n","import math\n","from pathlib import Path\n","import sys\n","\n","sys.path.insert(1, '/content/taming-transformers')\n","from IPython import display\n","from base64 import b64encode\n","from omegaconf import OmegaConf\n","from PIL import Image\n","from taming.models import cond_transformer, vqgan\n","import taming.modules\n","import torch\n","from torch import nn, optim\n","from torch.nn import functional as F\n","from torchvision import transforms\n","from torchvision.transforms import functional as TF\n","from tqdm.notebook import tqdm\n","\n","from CLIP import clip\n","import kornia.augmentation as K\n","import numpy as np\n","import imageio\n","from PIL import ImageFile, Image\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","def sinc(x):\n","    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n","\n","\n","def lanczos(x, a):\n","    cond = torch.logical_and(-a < x, x < a)\n","    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n","    return out / out.sum()\n","\n","\n","def ramp(ratio, width):\n","    n = math.ceil(width / ratio + 1)\n","    out = torch.empty([n])\n","    cur = 0\n","    for i in range(out.shape[0]):\n","        out[i] = cur\n","        cur += ratio\n","    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n","\n","\n","def resample(input, size, align_corners=True):\n","    n, c, h, w = input.shape\n","    dh, dw = size\n","\n","    input = input.view([n * c, 1, h, w])\n","\n","    if dh < h:\n","        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n","        pad_h = (kernel_h.shape[0] - 1) // 2\n","        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n","        input = F.conv2d(input, kernel_h[None, None, :, None])\n","\n","    if dw < w:\n","        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n","        pad_w = (kernel_w.shape[0] - 1) // 2\n","        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n","        input = F.conv2d(input, kernel_w[None, None, None, :])\n","\n","    input = input.view([n, c, h, w])\n","    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n","\n","\n","class ReplaceGrad(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, x_forward, x_backward):\n","        ctx.shape = x_backward.shape\n","        return x_forward\n","\n","    @staticmethod\n","    def backward(ctx, grad_in):\n","        return None, grad_in.sum_to_size(ctx.shape)\n","\n","\n","replace_grad = ReplaceGrad.apply\n","\n","\n","class ClampWithGrad(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, input, min, max):\n","        ctx.min = min\n","        ctx.max = max\n","        ctx.save_for_backward(input)\n","        return input.clamp(min, max)\n","\n","    @staticmethod\n","    def backward(ctx, grad_in):\n","        input, = ctx.saved_tensors\n","        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n","\n","\n","clamp_with_grad = ClampWithGrad.apply\n","\n","\n","def vector_quantize(x, codebook):\n","    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n","    indices = d.argmin(-1)\n","    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n","    return replace_grad(x_q, x)\n","\n","\n","class Prompt(nn.Module):\n","    def __init__(self, embed, weight=1., stop=float('-inf')):\n","        super().__init__()\n","        self.register_buffer('embed', embed)\n","        self.register_buffer('weight', torch.as_tensor(weight))\n","        self.register_buffer('stop', torch.as_tensor(stop))\n","\n","    def forward(self, input):\n","        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n","        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n","        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n","        dists = dists * self.weight.sign()\n","        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n","\n","\n","def parse_prompt(prompt):\n","    vals = prompt.rsplit(':', 2)\n","    vals = vals + ['', '1', '-inf'][len(vals):]\n","    return vals[0], float(vals[1]), float(vals[2])\n","\n","\n","class MakeCutouts(nn.Module):\n","    def __init__(self, cut_size, cutn, cut_pow=1.):\n","        super().__init__()\n","        self.cut_size = cut_size\n","        self.cutn = cutn\n","        self.cut_pow = cut_pow\n","\n","        self.augs = nn.Sequential(\n","            # K.RandomHorizontalFlip(p=0.5),\n","            # K.RandomVerticalFlip(p=0.5),\n","            # K.RandomSolarize(0.01, 0.01, p=0.7),\n","            # K.RandomSharpness(0.3,p=0.4),\n","            # K.RandomResizedCrop(size=(self.cut_size,self.cut_size), scale=(0.1,1),  ratio=(0.75,1.333), cropping_mode='resample', p=0.5),\n","            # K.RandomCrop(size=(self.cut_size,self.cut_size), p=0.5),\n","            K.RandomAffine(degrees=15, translate=0.1, p=0.7, padding_mode='border'),\n","            K.RandomPerspective(0.7,p=0.7),\n","            K.ColorJitter(hue=0.1, saturation=0.1, p=0.7),\n","            K.RandomErasing((.1, .4), (.3, 1/.3), same_on_batch=True, p=0.7),\n","\n",")\n","        self.noise_fac = 0.1\n","        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n","        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n","\n","    def forward(self, input):\n","        sideY, sideX = input.shape[2:4]\n","        max_size = min(sideX, sideY)\n","        min_size = min(sideX, sideY, self.cut_size)\n","        cutouts = []\n","\n","        for _ in range(self.cutn):\n","\n","            # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n","            # offsetx = torch.randint(0, sideX - size + 1, ())\n","            # offsety = torch.randint(0, sideY - size + 1, ())\n","            # cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n","            # cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n","\n","            # cutout = transforms.Resize(size=(self.cut_size, self.cut_size))(input)\n","\n","            cutout = (self.av_pool(input) + self.max_pool(input))/2\n","            cutouts.append(cutout)\n","        batch = self.augs(torch.cat(cutouts, dim=0))\n","        if self.noise_fac:\n","            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n","            batch = batch + facs * torch.randn_like(batch)\n","        return batch\n","\n","\n","def load_vqgan_model(config_path, checkpoint_path):\n","    config = OmegaConf.load(config_path)\n","    if config.model.target == 'taming.models.vqgan.VQModel':\n","        model = vqgan.VQModel(**config.model.params)\n","        model.eval().requires_grad_(False)\n","        model.init_from_ckpt(checkpoint_path)\n","    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n","        model = vqgan.GumbelVQ(**config.model.params)\n","        model.eval().requires_grad_(False)\n","        model.init_from_ckpt(checkpoint_path)\n","    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n","        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n","        parent_model.eval().requires_grad_(False)\n","        parent_model.init_from_ckpt(checkpoint_path)\n","        model = parent_model.first_stage_model\n","    else:\n","        raise ValueError(f'unknown model type: {config.model.target}')\n","    del model.loss\n","    return model\n","\n","\n","def resize_image(image, out_size):\n","    ratio = image.size[0] / image.size[1]\n","    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n","    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n","    return image.resize(size, Image.LANCZOS)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1tthw0YaispD"},"source":["## Settings for this run:\n","Mainly what you will have to modify will be `texts:`, there you can place the text or texts you want to generate (separated with `|`). It is a list because you can put more than one text, and so the AI ​​tries to 'mix' the images, giving the same priority to both texts.\n","\n","To use an initial image to the model, you just have to upload a file to the Colab environment (in the section on the left), and then modify `init_image:` putting the exact name of the file. Example: `sample.png`\n","\n","You can also modify the model by changing the lines that say `model:`. Currently ImageNet 1024, ImageNet 16384, WikiArt 1024, WikiArt 16384, S-FLCKR and COCO-Stuff are available. To activate them you have to have downloaded them first, and then you can simply select it.\n","\n","You can also use `target_images`, which is basically putting one or more images on it that the AI ​​will take as a \"target\", fulfilling the same function as putting text on it. To put more than one you have to use `|` as a separator."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ZdlpRFL8UAlW"},"outputs":[],"source":["#@title Parameters\n","texts = \"A state-of-the-art scientific laboratory with high-tech equipment, advanced chemical analysis tools, and clean sterile surfaces. \" #@param {type:\"string\"}\n","width =  720#@param {type:\"number\"}\n","height = 720#@param {type:\"number\"}\n","model = \"vqgan_imagenet_f16_16384\" #@param [\"vqgan_imagenet_f16_16384\", \"vqgan_imagenet_f16_1024\", \"vqgan_openimages_f16_8192\", \"wikiart_1024\", \"wikiart_16384\", \"coco\", \"faceshq\", \"sflckr\"]\n","images_interval =  10#@param {type:\"number\"}\n","init_image = \"photo_1403-07-24 12.08.28.jpeg\"#@param {type:\"string\"}\n","target_images = \"final_image.jpeg\"#@param {type:\"string\"}\n","seed = 142#@param {type:\"number\"}\n","max_iterations = 400#@param {type:\"number\"}\n","\n","model_names={\"vqgan_imagenet_f16_16384\": 'ImageNet 16384',\"vqgan_imagenet_f16_1024\":\"ImageNet 1024\", 'vqgan_openimages_f16_8192':'OpenImages 8912',\n","                 \"wikiart_1024\":\"WikiArt 1024\", \"wikiart_16384\":\"WikiArt 16384\", \"coco\":\"COCO-Stuff\", \"faceshq\":\"FacesHQ\", \"sflckr\":\"S-FLCKR\"}\n","name_model = model_names[model]\n","\n","if seed == -1:\n","    seed = None\n","if init_image == \"None\":\n","    init_image = None\n","if target_images == \"None\" or not target_images:\n","    target_images = []\n","else:\n","    target_images = target_images.split(\"|\")\n","    target_images = [image.strip() for image in target_images]\n","\n","texts = [phrase.strip() for phrase in texts.split(\"|\")]\n","if texts == ['']:\n","    texts = []\n","\n","\n","args = argparse.Namespace(\n","    prompts=texts,\n","    image_prompts=target_images,\n","    noise_prompt_seeds=[],\n","    noise_prompt_weights=[],\n","    size=[width, height],\n","    init_image=init_image,\n","    init_weight=0.,\n","    clip_model='ViT-B/32',\n","    vqgan_config=f'{model}.yaml',\n","    vqgan_checkpoint=f'{model}.ckpt',\n","    step_size=0.1,\n","    cutn=32,\n","    cut_pow=1.,\n","    display_freq=images_interval,\n","    seed=seed,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJw8tdWlOWKS"},"outputs":[],"source":["from ipywidgets import FloatProgress"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["827783b1b338401ab99af299e6d0f03d","7dc5641ced3042f39852b7c4b4d4866e","39b7841e87374d7dbfce0111dd92be0e","e65a353732d34138994b98bbae22d4d0","a7946d0f73f5489895cf4a14bd3dae2e","a8d9cd822c604c49bb3d60bd9946ac7b","c8df6645b51e4ac786533800da03232c","90a8c180726c4c1380ed33510b814e51","042957194f184776b198e7e1a662dd84","5cb1557f324249ebb27640c10e6b6569","e161e448ac244f0bad2e81df70312784"]},"id":"JX56bq4rEKIp","outputId":"ce548719-ff1b-4526-889b-09a82331b7e7"},"outputs":[],"source":["#@title Actually do the run...\n","from urllib.request import urlopen\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","if texts:\n","    print('Using texts:', texts)\n","if target_images:\n","    print('Using image prompts:', target_images)\n","if args.seed is None:\n","    seed = torch.seed()\n","else:\n","    seed = args.seed\n","torch.manual_seed(seed)\n","print('Using seed:', seed)\n","\n","model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n","perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n","# clock=deepcopy(perceptor.visual.positional_embedding.data)\n","# perceptor.visual.positional_embedding.data = clock/clock.max()\n","# perceptor.visual.positional_embedding.data=clamp_with_grad(clock,0,1)\n","\n","cut_size = perceptor.visual.input_resolution\n","\n","f = 2**(model.decoder.num_resolutions - 1)\n","make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n","\n","toksX, toksY = args.size[0] // f, args.size[1] // f\n","sideX, sideY = toksX * f, toksY * f\n","\n","if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n","    e_dim = 256\n","    n_toks = model.quantize.n_embed\n","    z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n","    z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n","else:\n","    e_dim = model.quantize.e_dim\n","    n_toks = model.quantize.n_e\n","    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n","    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n","# z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n","# z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n","\n","# normalize_imagenet = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","#                                            std=[0.229, 0.224, 0.225])\n","\n","if args.init_image:\n","    if 'http' in args.init_image:\n","      img = Image.open(urlopen(args.init_image))\n","    else:\n","      img = Image.open(args.init_image)\n","    pil_image = img.convert('RGB')\n","    pil_image = pil_image.resize((sideX, sideY), Image.LANCZOS)\n","    pil_tensor = TF.to_tensor(pil_image)\n","    z, *_ = model.encode(pil_tensor.to(device).unsqueeze(0) * 2 - 1)\n","else:\n","    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n","    # z = one_hot @ model.quantize.embedding.weight\n","    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n","        z = one_hot @ model.quantize.embed.weight\n","    else:\n","        z = one_hot @ model.quantize.embedding.weight\n","    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n","    z = torch.rand_like(z)*2\n","z_orig = z.clone()\n","z.requires_grad_(True)\n","opt = optim.Adam([z], lr=args.step_size)\n","\n","normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n","                                  std=[0.26862954, 0.26130258, 0.27577711])\n","\n","\n","\n","pMs = []\n","\n","for prompt in args.prompts:\n","    txt, weight, stop = parse_prompt(prompt)\n","    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n","    pMs.append(Prompt(embed, weight, stop).to(device))\n","\n","for prompt in args.image_prompts:\n","    path, weight, stop = parse_prompt(prompt)\n","    img = Image.open(path)\n","    pil_image = img.convert('RGB')\n","    img = resize_image(pil_image, (sideX, sideY))\n","    batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n","    embed = perceptor.encode_image(normalize(batch)).float()\n","    pMs.append(Prompt(embed, weight, stop).to(device))\n","\n","for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n","    gen = torch.Generator().manual_seed(seed)\n","    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n","    pMs.append(Prompt(embed, weight).to(device))\n","\n","def synth(z):\n","    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n","        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n","    else:\n","        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n","    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n","\n","@torch.no_grad()\n","def checkin(i, losses):\n","    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n","    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n","    out = synth(z)\n","    TF.to_pil_image(out[0].cpu()).save('progress.png')\n","    display.display(display.Image('progress.png'))\n","\n","def ascend_txt():\n","    global i\n","    out = synth(z)\n","    iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n","\n","    result = []\n","\n","    if args.init_weight:\n","        # result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n","        result.append(F.mse_loss(z, torch.zeros_like(z_orig)) * ((1/torch.tensor(i*2 + 1))*args.init_weight) / 2)\n","    for prompt in pMs:\n","        result.append(prompt(iii))\n","    img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n","    img = np.transpose(img, (1, 2, 0))\n","    imageio.imwrite('./steps/' + str(i) + '.png', np.array(img))\n","\n","    return result\n","\n","def train(i):\n","    opt.zero_grad()\n","    lossAll = ascend_txt()\n","    if i % args.display_freq == 0:\n","        checkin(i, lossAll)\n","\n","    loss = sum(lossAll)\n","    loss.backward()\n","    opt.step()\n","    with torch.no_grad():\n","        z.copy_(z.maximum(z_min).minimum(z_max))\n","\n","i = 0\n","try:\n","    with tqdm() as pbar:\n","        while True:\n","            train(i)\n","            if i == max_iterations:\n","                break\n","            i += 1\n","            pbar.update()\n","except KeyboardInterrupt:\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6b0EJIRLVwAE","outputId":"05f28e64-f8b2-492a-da82-7ce1cf95eaed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: ffmpeg in /home/avidmech/miniconda3/lib/python3.12/site-packages (1.4)\r\n"]}],"source":["!pip install ffmpeg"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DTdC_tnhbair","outputId":"ebe74d03-187c-4b6d-d4eb-470ae4dafd2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[sudo] password for avidmech: \n"]}],"source":["!sudo apt install ffmpeg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9R60CcDpbrll"},"outputs":[],"source":["import ffmpeg"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["cf17c31982d24f21b3be504d589633be","0bfba86b1970401dab0a31d61a0cb1d0","57f7a841f980411e891d4daa0be1c6bc","73eed4338e084ac48f57a42fd6261ea2","7ef0c44fe642418f83d39e76fc866306","96012db5799147bdbcd68e27f1ae354b","44152eefb64a4c219b68d1a609f157b6","a2ddcfcd5cac47c0852708262ea74c05","142a90d4c5ca4c07817f3dafa1d083a6","fad230cc32664f349aea92132ed4dbd4","877e22df7b0a43d9905c9d7317887cef"]},"id":"gmK0k5zQeT5u","outputId":"c656059e-0405-4123-b334-4dd7bde24db9"},"outputs":[],"source":["#@title Generate a video with the result\n","\n","init_frame = 1 #This is the frame where the video will start\n","last_frame = i #You can change i to the number of the last frame you want to generate. It will raise an error if that number of frames does not exist.\n","\n","min_fps = 10\n","max_fps = 30\n","\n","total_frames = last_frame-init_frame\n","\n","length = 13 #Desired time of the video in seconds\n","\n","frames = []\n","tqdm.write('Generating video...')\n","for i in range(init_frame,last_frame): #\n","    frames.append(Image.open(\"./steps/\"+ str(i) +'.png'))\n","\n","#fps = last_frame/10\n","fps = np.clip(total_frames/length,min_fps,max_fps)\n","\n","from subprocess import Popen, PIPE\n","p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', 'video.mp4'], stdin=PIPE)\n","for im in tqdm(frames):\n","    im.save(p.stdin, 'PNG')\n","p.stdin.close()\n","p.wait()\n","mp4 = open('video.mp4','rb').read()\n","data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","display.HTML(\"\"\"\n","<video width=400 controls>\n","      <source src=\"%s\" type=\"video/mp4\">\n","</video>\n","\"\"\" % data_url)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"042957194f184776b198e7e1a662dd84":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0bfba86b1970401dab0a31d61a0cb1d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_96012db5799147bdbcd68e27f1ae354b","placeholder":"​","style":"IPY_MODEL_44152eefb64a4c219b68d1a609f157b6","tabbable":null,"tooltip":null,"value":"100%"}},"142a90d4c5ca4c07817f3dafa1d083a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"39b7841e87374d7dbfce0111dd92be0e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_90a8c180726c4c1380ed33510b814e51","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_042957194f184776b198e7e1a662dd84","tabbable":null,"tooltip":null,"value":1}},"44152eefb64a4c219b68d1a609f157b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"57f7a841f980411e891d4daa0be1c6bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_a2ddcfcd5cac47c0852708262ea74c05","max":399,"min":0,"orientation":"horizontal","style":"IPY_MODEL_142a90d4c5ca4c07817f3dafa1d083a6","tabbable":null,"tooltip":null,"value":399}},"5cb1557f324249ebb27640c10e6b6569":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73eed4338e084ac48f57a42fd6261ea2":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_fad230cc32664f349aea92132ed4dbd4","placeholder":"​","style":"IPY_MODEL_877e22df7b0a43d9905c9d7317887cef","tabbable":null,"tooltip":null,"value":" 399/399 [01:20&lt;00:00,  5.53it/s]"}},"7dc5641ced3042f39852b7c4b4d4866e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_a8d9cd822c604c49bb3d60bd9946ac7b","placeholder":"​","style":"IPY_MODEL_c8df6645b51e4ac786533800da03232c","tabbable":null,"tooltip":null,"value":""}},"7ef0c44fe642418f83d39e76fc866306":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"827783b1b338401ab99af299e6d0f03d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7dc5641ced3042f39852b7c4b4d4866e","IPY_MODEL_39b7841e87374d7dbfce0111dd92be0e","IPY_MODEL_e65a353732d34138994b98bbae22d4d0"],"layout":"IPY_MODEL_a7946d0f73f5489895cf4a14bd3dae2e","tabbable":null,"tooltip":null}},"877e22df7b0a43d9905c9d7317887cef":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"90a8c180726c4c1380ed33510b814e51":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"96012db5799147bdbcd68e27f1ae354b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2ddcfcd5cac47c0852708262ea74c05":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7946d0f73f5489895cf4a14bd3dae2e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8d9cd822c604c49bb3d60bd9946ac7b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8df6645b51e4ac786533800da03232c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"cf17c31982d24f21b3be504d589633be":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0bfba86b1970401dab0a31d61a0cb1d0","IPY_MODEL_57f7a841f980411e891d4daa0be1c6bc","IPY_MODEL_73eed4338e084ac48f57a42fd6261ea2"],"layout":"IPY_MODEL_7ef0c44fe642418f83d39e76fc866306","tabbable":null,"tooltip":null}},"e161e448ac244f0bad2e81df70312784":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"e65a353732d34138994b98bbae22d4d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_5cb1557f324249ebb27640c10e6b6569","placeholder":"​","style":"IPY_MODEL_e161e448ac244f0bad2e81df70312784","tabbable":null,"tooltip":null,"value":" 400/? [04:01&lt;00:00,  1.91it/s]"}},"fad230cc32664f349aea92132ed4dbd4":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
